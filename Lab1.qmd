---
title: "Lab1"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
embed-resources: true
editor: visual
theme: minty
toc: true
---

```{r, message = FALSE}
library(tidyverse)
library(lubridate)
```

# Exercises

## Question 1

Tablet devices are an increasingly important component of the global electronics market. According to a market intelligence research company, the use of tablet devices can be classified into the following user segments.

```{=tex}
\begin{array}{lcc}
\text{User Segment} & 2012 ~ \text{percentages} &  \text{Current survey frequency}\\ \hline
\text{Business-Professional} &          69\% &  102  \\
\text{Goverment} &   21\%         & 32           \\
\text{Education}  &      7\% &     12       \\
\text{Home} & 3\% & 4 \\  \hline
\text{Total} & 100\% & 150 \\ \hline
\end{array}
```
Do the data provide sufficient evidence to indicate that the figures obtained in the current survey agree with the percentages in 2012?

## Answer:

Significance level: $\alpha = 0.01$.

Hypothesis:

$H_0$ = The figures obtained in the current survey agree with the percentages in 2012.

$H_1$ = The figures obtained in the current survey do not match the percentages in 2012.

Assumptions:

Expected frequencies are sufficiently large. The expectation for "Home" is less than 5, but not drastically enough to influence the test statistic. In future, "Home" and "Education" could be merged to rectify this.

Table:

```{r}
y_i = c(102, 32, 12, 4)
p_i = c(0.69, 0.21, 0.07, 0.03)
n = sum(y_i)
e_i = n * p_i
e_i = p_i * n
d_i = ((y_i - e_i)^2)/e_i

df = tibble(
  User = c("Business-Professional", "Government", "Education", "Home"),
  y = y_i,
  p = p_i,
  e = e_i,
  Contribution = d_i
)
df

t0 = sum(df$Contribution)
```

Test statistic:

```{r}
print(t0)
```

Using the chi-squared test with 3 degrees of freedom:

```{r}
pchisq(t0, df = 3, lower.tail = FALSE)
```

The p-value obtained is greater than $\alpha$, hence we retain the null hypothesis. Thus, the data appears to provide sufficient evidence to indicate that the figures of the current survey agree with the percentages of those in 2012.

```{r}
barplot(df$y, names.arg = df$User, main = 'Observed frequencies')
barplot(df$e, names.arg = df$User, main = 'Expected frequencies')
```

## Question 2

A study of patients with insulin-dependent diabetes was conducted to investigate the effects of cigarette smoking on renal and retinal complications. Before examining the results of the study, a researcher expects that the proportions of four different subgroups are as follow:

```{=tex}
\begin{array} {lcc}
\text{Subgroup} & \text{Proportion} \\ \hline   
\text{Nonsmokers} & 0.50 \\
\text{Current Smokers} & 0.20 \\
\text{Tobacco Chewers} & 0.10 \\
\text{Ex-smokers} & 0.20 \\ \hline
\end{array}
```
Of 100 randomly selected patients, there are 44 nonsmokers, 24 current smokers, 13 tobacco chewers and 19 ex-smokers. Should the researcher revise his estimates? Use 0.01 as the level of significance.

## Answer

Significance level: $\alpha = 0.01$.

Hypothesis:

$H_0$ = The distribution of patients obeys the estimated proportions.

$H_1$ = The distribution of patients does not obey the estimated proportions.

Assumptions:

All expected frequencies are sufficiently large (\>5).

Table:

```{r}
y_i = c(44, 24, 13, 19)
p_i = c(0.5, 0.2, 0.1, 0.2)
n = sum(y_i)
e_i = n * p_i
d_i = ((y_i - e_i)^2)/e_i

df = tibble(
  Subgroup = c("Nonsmokers", "Current Smokers", "Tobacco Chewers", "Ex-smokers"),
  y = y_i,
  p = p_i,
  e = e_i,
  Contribution = d_i
)
df
```

Test statistic:

```{r}
t = sum(df$Contribution)
t
```

Using the chi-squared test with 3 degrees of freedom:

```{r}
pchisq(t, df = 3, lower.tail = FALSE)
```

The p-value obtained is greater than $\alpha$, hence we retain the null hypothesis. Thus, the researcher likely does not need to revise his estimates.

```{r}
barplot(df$y, names.arg = df$Subgroup, main = 'Observed frequencies')
barplot(df$e, names.arg = df$Subgroup, main = 'Expected frequencies')
```

# Australian Road Fatalities

```{r}
# fatalities data
fdata = readxl::read_excel("bitre_fatalities_jun2024.xlsx", 
                           sheet = 2, 
                           skip = 4, 
                           na = c("","-9"), 
                           guess_max = 1e6) |> 
  janitor::clean_names()

# crash data
cdata = fdata |> 
  dplyr::select(-road_user, -gender, -age, -age_group) |> 
  dplyr::distinct() |> 
  dplyr::group_by(crash_id) |> 
  dplyr::slice(1) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(hour = lubridate::hour(time))
```

1\) How are missing values recorded, and why might they occur?

Missing values are recorded as a "-9", and may occur because the index states that the data is provided by "third parties", which may withhold or fail to provide certain details e.g. age or vehicle involvement.

2\) How many fatalities occurred since 1989? How many fatal crashes have there been since 1989?

```{r}
dim(cdata)
str(cdata)
```

There have been 50, 682 fatal crashes since 1989.

```{r}
length(fdata$crash_id)
```

There have been 56, 218 fatalities since 1989.

3.  What is the most common hour of the day for a fatal crash?

```{r}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

Mode(cdata$hour)
```

4.  What is the most common day of the week for a fatal crash?

```{r}
Mode(cdata$dayweek)
```

5.  What is the most common month for a fatal crash?

```{r}
Mode(cdata$month)
```

6.  Are fatal crashes uniformly distributed across the months of the year? Filter the data down to one year (e.g.Â 2019) to do this test. You should write out a full hypothesis test and make an appropriate conclusion.

Significance level: $\alpha = 0.05$.

Hypothesis:

$H_0:$ The distribution of fatal crashes across the months of the year is uniform.

$H_1:$ The distribution of fatal crashes across the months of the year is not uniform.

Assumptions:

The expected frequencies are sufficiently large (\>5).

Table:

```{r}
y_i = tabulate(match(cdata$month, unique(cdata$month)))
n = sum(y_i)
p_i = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)/12
e_i = n * p_i
d_i = ((y_i-e_i)^2)/e_i

df = tibble(
  Months = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"),
  y = y_i,
  p = p_i,
  e = e_i,
  Contribution = d_i
)
df

t = sum(df$Contribution)
```

Test statistic:

```{r, echo = FALSE}
t
```

Using the chi-squared distribution with 11 degrees of freedom:

```{r, echo = FALSE}
pchisq(t, df = 11, lower.tail = FALSE)
```

Using the significance level of $\alpha = 0.05$, the p-value is drastically smaller. Thus, we reject the null hypothesis. Consequently, there is a statistically significant deviation from a uniform distribution, hence the fatal crashes are not uniformly distributed across the months of the year.
